{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection\n",
    "Feature Selection 방법은 Machine Learning 에 있어 굉장히 비중이 큰 작업이다.\n",
    "데이터가 굉장히 많고, 다양하게 존재한다면, Featuer Selection이 크게 중요하지 않을 수 있다.\n",
    "하지만, 현실 세계에서의 데이터는 이상적이지 않기 때문에, 중요한 변수의 선택의 필요성이 급증하게 되었다.\n",
    "결과를 예측하는데 상관 없는 변수들이 존재하면, Computing Cost가 늘어나고, Overfitting을 초래할 수 있다.\n",
    "\n",
    "Feature Selection의 장점은 모델의 성능에 영향을 주는 특징들만 선택하기 때문에, 학습 시간을 줄일 수 있고,\n",
    "모델의 분산을 줄임으로써, 보다 Robust하게 학습을 할 수 있게 된다. 그리고 모델의 학습이 간소해지기 때문에\n",
    "결과에 대한 해석이 쉬워진다.\n",
    "\n",
    "Feature Selection의 주목적은 독립 변수 중에서, 중복되거나 종속변수와 관련이 없는 변수들을 제거하여, 종속변수를 가장 잘 예측하는 변수들의\n",
    "좋바을 찾아내는 것이기 때문에, 최적화 문제로도 정의할 수 있다.\n",
    "즉, 모델의 학습효율을 최대로 하는 Data의 패턴을 찾아내는 것이 Feauter Selection의 역할이다.\n",
    "\n",
    "Feature Selection은 Feature Engineering, Feature Extraction과 유사하지만, 표현 자체는 구분되며, 간단하게 정리하면 다음과 같다.\n",
    "* Feature Engineering : 도메인 지식을 사용하여 데이터에서 피처를 변형/생성하는 방법\n",
    "* Feature Extraction : 차원축소 등 새로운 중요 피쳐를 추출하는 방법\n",
    "* Feature Selection : 기존 피쳐에서 원하는 피쳐만(변경하지 않고) 선택하는 방법\n",
    "\n",
    "\n",
    "즉, Feature Engineering&Extraction은 데이터의 피쳐를 어떻게 유용하게 만들것인가의 문제이고,\n",
    "Feature Selection은 데이터에서 유용한 피쳐를 어떻게 선택할 것인가의 문제이다.\n",
    "\n",
    "\n",
    "Feature Selection을 한다는 것은 하위 셋을 만들기 위한 과정이기에 시간과 자원이 충분하다면 $2^n - 1$가지 방법을 모두 테스트하여 구하고자 하는 score가 높은 subset을\n",
    "사용하면 된다. 하지만 이방법은 현실적으로 무리기 때문에 평가 메트릭에 따라 적합한 방법을 사용하는 것이 좋다.\n",
    "\n",
    "Feature Selection 방법을 크게 분류를 하자면 **Filtering, Wrapper, Embedded** 세 가지 방법이 있다.\n",
    "\n",
    "\n",
    "#### 1. Wrapper Method : 유용성을 측정한 방법\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Feature_selection_Wrapper_Method.png/600px-Feature_selection_Wrapper_Method.png\" alt=\"Wrapper Method\">\n",
    "\n",
    "**Wrapper Method**는 예측 모델을 사용하여 Feature Subset을 계쏙 테스트한다. 이경우 기존 데이터에서 Cross Validaion을 위한 hold-out set을 따로두어야한다.\n",
    "이렇게 Subset을 체크하면 어떤 Feature가 필요한지 알 수 있다.\n",
    "최종적으로 Best Feature Subset을 갖기 때문에 모델의 성능을 위해 매우 바람직한 방법이나, 설계된 모델의 파라미터와 알고리즘 자체의 완성도가 높아야한다는 전제 조건이 따른다.\n",
    "그러나 이 방법은 Computing power가 비약적으로 많이 들기 때문에 random hill-climbing과 같은 휴리스틱 방법론을 사용한다.\n",
    "\n",
    "* Recursive Feature Elimination(RFE) ([Link](https://link.springer.com/article/10.1023%2FA%3A1012487302797))\n",
    " * scikit-leanr에 [함수](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)가 있다.<br>\n",
    " * SVM을 사용하여 재귀적으로 제거하는 방법<br>\n",
    " * 유사한 방법으로 Backward Elimination, Forward Elimination, Bidirectional Elimination이 있다.<br>\n",
    "\n",
    "* Sequential Feature Selection(SFS)\n",
    "  * mlxtend에 [함수](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)가 있다.<br>\n",
    "  * Greedy 알고리즘으로 빈 Subset에서 Feature를 하나씩 추가하는 방법으로 이루어진다.<br>\n",
    "* Genetic Algorithm\n",
    "* Univariate Selection\n",
    "* Exhaustive\n",
    "* mRMR(Minimum Redundancy Maximum Relevance)<br>\n",
    "  * 피처의 중복성을 최소화하여 Relevancy를 최대화하는 방법<br>\n",
    "  * [Three Effective Feature Selection Strategies](https://medium.com/ai%C2%B3-theory-practice-business/three-effective-feature-selection-strategies-e1f86f331fb1) 글을 참고하는 것을 추천한다.\n",
    "\n",
    "#### 2. Filter Method(관련성을 찾는 방법)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Filter_Methode.png/600px-Filter_Methode.png\" alt=\"Filter method\">\n",
    "Filter Method는 통계적 측정 방법을 사용하여 Feature들의 관계를 알아내는 방법이다. 대표적으로 사용되는 통계적 방법은 상관계수를 보는 것이다.\n",
    "하지만 상관계수가 반드시 모델에 적합한 피처라고는 할 수 없고, Subset의 조정이 정확하지 않다. 대신 계산속도가 빠르고 Feature간 상관관계를 알아내는데 적합하기 때문에 Wrapper\n",
    "Method를 사용하기 전에 전처리하는 용도로 사용할 수 있다. 아래와 같은 방법을 사용한다.\n",
    "* Information Gain\n",
    "* Chi-square Test\n",
    "* Fisher Score\n",
    "* Correlation Coefficient\n",
    "* Variance Threshold\n",
    "\n",
    "#### 3. Embedded Method(유용성을 측정하지만 내장 Metric을 사용하는 방법)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Feature_selection_Embedded_Method.png/600px-Feature_selection_Embedded_Method.png\" alt=\"Embedded Method\">\n",
    "\n",
    "**Embedded Method**는 모델의 정확도에 기여하는 Feature를 학습하는 방법이다. 좀 더 적은 계수를 가지는 회귀식을 찾는 방향으로 제약조건을 주어 이를 제어한다. 사용되는 방법들은 다음과 같다\n",
    "* LASSO : L1-norm을 통해 제약을 주는 방법\n",
    "* RIDGE : L2-norm을 통해 제약을 주는 방법\n",
    "* Elastic Net :  위 두 방법을 선형결합한 방법\n",
    "* SelectFromModel\n",
    " * Decision Tree 기반 알고리즘에서 Feature를 뽑아오는 방법(RandomForest, LightGBM 등)\n",
    " * Scikit-Learn에 [함수](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)가 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[참고자료]\n",
    "\n",
    "1. [Begginer Guide : Feature Selection](https://subinium.github.io/feature-selection/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}