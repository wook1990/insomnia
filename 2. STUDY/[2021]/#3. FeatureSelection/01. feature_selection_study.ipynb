{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Selection\r\n",
    "Feature Selection 방법은 Machine Learning 에 있어 굉장히 비중이 큰 작업이다.\r\n",
    "데이터가 굉장히 많고, 다양하게 존재한다면, Featuer Selection이 크게 중요하지 않을 수 있다.\r\n",
    "하지만, 현실 세계에서의 데이터는 이상적이지 않기 때문에, 중요한 변수의 선택의 필요성이 급증하게 되었다.\r\n",
    "결과를 예측하는데 상관 없는 변수들이 존재하면, Computing Cost가 늘어나고, Overfitting을 초래할 수 있다.\r\n",
    "\r\n",
    "Feature Selection의 장점은 모델의 성능에 영향을 주는 특징들만 선택하기 때문에, 학습 시간을 줄일 수 있고,\r\n",
    "모델의 분산을 줄임으로써, 보다 Robust하게 학습을 할 수 있게 된다. 그리고 모델의 학습이 간소해지기 때문에\r\n",
    "결과에 대한 해석이 쉬워진다. 정리하자면 다음과 같다.  \r\n",
    "\r\n",
    "- Feature 수가 적은 모델은 설명하기 쉽다.\r\n",
    "- Feature가 축소된 모델을 구현하는 것이 더 쉽다.\r\n",
    "- Feature가 적을 수록 모델이 일반화되어 과적합문제가 감소한다.\r\n",
    "- Feature Selection 을 통해 중복 데이터를 제거할 수 있다.\r\n",
    "- Feature 수가 줄어들어 모델의 훈련시간이 단축된다.\r\n",
    "- Feature가 적은 모델일 수록 오류 발생 가능성이 낮아진다.\r\n",
    "\r\n",
    "\r\n",
    "Feature Selection의 주목적은 독립 변수 중에서, 중복되거나 종속변수와 관련이 없는 변수들을 제거하여, 종속변수를 가장 잘 예측하는 변수들의\r\n",
    "좋바을 찾아내는 것이기 때문에, 최적화 문제로도 정의할 수 있다.\r\n",
    "즉, 모델의 학습효율을 최대로 하는 Data의 패턴을 찾아내는 것이 Feauter Selection의 역할이다.\r\n",
    "\r\n",
    "Feature Selection은 Feature Engineering, Feature Extraction과 유사하지만, 표현 자체는 구분되며, 간단하게 정리하면 다음과 같다.\r\n",
    "\r\n",
    "* Feature Engineering : 도메인 지식을 사용하여 데이터에서 피처를 변형/생성하는 방법\r\n",
    "* Feature Extraction : 차원축소 등 새로운 중요 피쳐를 추출하는 방법\r\n",
    "* Feature Selection : 기존 피쳐에서 원하는 피쳐만(변경하지 않고) 선택하는 방법\r\n",
    "\r\n",
    "<img alt=\"\" class=\"nk tn t u v jn aj c\" width=\"700\" height=\"455\" role=\"presentation\" src=\"https://miro.medium.com/max/700/1*Deb-gxVzJdR6QdBw-AQ2ug.jpeg\" srcset=\"https://miro.medium.com/max/276/1*Deb-gxVzJdR6QdBw-AQ2ug.jpeg 276w, https://miro.medium.com/max/552/1*Deb-gxVzJdR6QdBw-AQ2ug.jpeg 552w, https://miro.medium.com/max/640/1*Deb-gxVzJdR6QdBw-AQ2ug.jpeg 640w, https://miro.medium.com/max/700/1*Deb-gxVzJdR6QdBw-AQ2ug.jpeg 700w\" sizes=\"700px\">\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "즉, Feature Engineering&Extraction은 데이터의 피쳐를 어떻게 유용하게 만들것인가의 문제이고,\r\n",
    "Feature Selection은 데이터에서 유용한 피쳐를 어떻게 선택할 것인가의 문제이다.\r\n",
    "\r\n",
    "\r\n",
    "Feature Selection을 한다는 것은 하위 셋을 만들기 위한 과정이기에 시간과 자원이 충분하다면 $2^n - 1$가지 방법을 모두 테스트하여 구하고자 하는 score가 높은 subset을\r\n",
    "사용하면 된다. 하지만 이방법은 현실적으로 무리기 때문에 평가 메트릭에 따라 적합한 방법을 사용하는 것이 좋다.\r\n",
    "\r\n",
    "Feature Selection 방법을 크게 분류를 하자면 **Filtering, Wrapper, Embedded** 세 가지 방법이 있다.\r\n",
    "\r\n",
    "\r\n",
    "#### 1. Wrapper Method(유용성을 측정한 방법)\r\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Feature_selection_Wrapper_Method.png/600px-Feature_selection_Wrapper_Method.png\" alt=\"Wrapper Method\">\r\n",
    "\r\n",
    "**Wrapper Method**는 예측 모델을 사용하여 Feature Subset을 계쏙 테스트한다. 이경우 기존 데이터에서 Cross Validaion을 위한 hold-out set을 따로두어야한다.\r\n",
    "이렇게 Subset을 체크하면 어떤 Feature가 필요한지 알 수 있다.\r\n",
    "최종적으로 Best Feature Subset을 갖기 때문에 모델의 성능을 위해 매우 바람직한 방법이나, 설계된 모델의 파라미터와 알고리즘 자체의 완성도가 높아야한다는 전제 조건이 따른다.\r\n",
    "그러나 이 방법은 Computing power가 비약적으로 많이 들기 때문에 random hill-climbing과 같은 휴리스틱 방법론을 사용한다.\r\n",
    "\r\n",
    "* Recursive Feature Elimination(RFE) ([Link](https://link.springer.com/article/10.1023%2FA%3A1012487302797))\r\n",
    " * scikit-leanr에 [함수](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)가 있다.<br>\r\n",
    " * SVM을 사용하여 재귀적으로 제거하는 방법<br>\r\n",
    " * 유사한 방법으로 Backward Elimination, Forward Elimination, Bidirectional Elimination이 있다.<br>\r\n",
    "\r\n",
    "* Sequential Feature Selection(SFS)\r\n",
    "  * mlxtend에 [함수](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)가 있다.<br>\r\n",
    "  * Greedy 알고리즘으로 빈 Subset에서 Feature를 하나씩 추가하는 방법으로 이루어진다.<br>\r\n",
    "* Genetic Algorithm\r\n",
    "* Univariate Selection\r\n",
    "* Exhaustive\r\n",
    "* mRMR(Minimum Redundancy Maximum Relevance)<br>\r\n",
    "  * 피처의 중복성을 최소화하여 Relevancy를 최대화하는 방법<br>\r\n",
    "  * [Three Effective Feature Selection Strategies](https://medium.com/ai%C2%B3-theory-practice-business/three-effective-feature-selection-strategies-e1f86f331fb1) 글을 참고하는 것을 추천한다.\r\n",
    "\r\n",
    "#### 2. Filter Method(관련성을 찾는 방법)\r\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Filter_Methode.png/600px-Filter_Methode.png\" alt=\"Filter method\">\r\n",
    "Filter Method는 통계적 측정 방법을 사용하여 Feature들의 관계를 알아내는 방법이다. 대표적으로 사용되는 통계적 방법은 상관계수를 보는 것이다.\r\n",
    "하지만 상관계수가 반드시 모델에 적합한 피처라고는 할 수 없고, Subset의 조정이 정확하지 않다. 대신 계산속도가 빠르고 Feature간 상관관계를 알아내는데 적합하기 때문에 Wrapper\r\n",
    "Method를 사용하기 전에 전처리하는 용도로 사용할 수 있다. 아래와 같은 방법을 사용한다.\r\n",
    "* Information Gain\r\n",
    "* Chi-square Test\r\n",
    "* Fisher Score\r\n",
    "* Correlation Coefficient\r\n",
    "* Variance Threshold\r\n",
    "\r\n",
    "#### 3. Embedded Method(유용성을 측정하지만 내장 Metric을 사용하는 방법)\r\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Feature_selection_Embedded_Method.png/600px-Feature_selection_Embedded_Method.png\" alt=\"Embedded Method\">\r\n",
    "\r\n",
    "**Embedded Method**는 모델의 정확도에 기여하는 Feature를 학습하는 방법이다. 좀 더 적은 계수를 가지는 회귀식을 찾는 방향으로 제약조건을 주어 이를 제어한다. 사용되는 방법들은 다음과 같다\r\n",
    "* [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics): L1-norm을 통해 제약을 주는 방법\r\n",
    "* [RIDGE](https://en.wikipedia.org/wiki/Tikhonov_regularization) : L2-norm을 통해 제약을 주는 방법\r\n",
    "* [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization) :  위 두 방법을 선형결합한 방법\r\n",
    "* [SelectFromModel](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2)\r\n",
    " * Decision Tree 기반 알고리즘에서 Feature를 뽑아오는 방법(RandomForest, LightGBM 등)\r\n",
    " * Scikit-Learn에 [함수](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)가 있다.\r\n",
    "\r\n",
    "\r\n",
    "#### 주의점\r\n",
    "\r\n",
    "___\r\n",
    "* 훈련 데이터에서 Feature를 고른다면, 훈련 데이터에 과적합될 수 있기 때문에, 테스트 데이터를 분리한뒤 훈련 데이터에서 선택하는 것이 중요하다.\r\n",
    "\r\n",
    "\r\n",
    "[참고자료]\r\n",
    "\r\n",
    "1. [Begginer Guide : Feature Selection](https://subinium.github.io/feature-selection/) - An Subin\r\n",
    "2. [An Introduction to Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/) -  Jason Brownlee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('analysis': conda)",
   "name": "pycharm-58e97ac5"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}